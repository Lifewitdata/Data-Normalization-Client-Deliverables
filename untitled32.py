# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14BNMGMKr7RkbrfSVLwo_lfC2z1vK2KU8
"""

import pandas as pd
import numpy as np

# Load the three CSVs
df1 = pd.read_csv("/content/raw_orders_sourceA.csv")
df2 = pd.read_csv("/content/raw_orders_sourceB.csv")
df3 = pd.read_csv("/content/raw_orders_sourceC.csv")

# --- Source A Cleaning ---
df1_clean = pd.DataFrame()
df1_clean["OrderID"] = df1["OrderID"]
df1_clean["CustomerName"] = df1["Cust_Name"].str.title().str.strip()
df1_clean["OrderDate"] = pd.to_datetime(df1["OrderDate"], errors="coerce", dayfirst=False)
df1_clean["AmountUSD"] = pd.to_numeric(df1["AmountUSD"], errors="coerce")
df1_clean["Email"] = df1["Email"].str.strip().str.lower()

# --- Source C Cleaning ---
df3_clean = pd.DataFrame()
df3_clean["OrderID"] = df3["TransactionID"]
df3_clean["CustomerName"] = df3["ClientName"].str.title().str.strip()
df3_clean["OrderDate"] = pd.to_datetime(df3["TxnDate"], errors="coerce", dayfirst=False)
df3_clean["AmountUSD"] = pd.to_numeric(df3["ValueUSD"], errors="coerce")
df3_clean["Email"] = df3["EmailAddress"].str.strip().str.lower()

df3_clean = pd.DataFrame()
df3_clean["OrderID"] = df3["TransactionID"]
df3_clean["CustomerName"] = df3["ClientName"].str.title().str.strip()
df3_clean["OrderDate"] = pd.to_datetime(df3["TxnDate"], errors="coerce", dayfirst=False)

df3_clean["AmountUSD"] = pd.to_numeric(df3["ValueUSD"].str.replace(",", ""), errors="coerce")
df3_clean["Email"] = df3["EmailAddress"].str.strip().str.lower()

# --- Source B Cleaning ---
df2_clean = pd.DataFrame()
df2_clean["OrderID"] = df2["ID"]
df2_clean["CustomerName"] = df2["Customer"].str.title().str.strip()
df2_clean["OrderDate"] = pd.to_datetime(df2["Date"], errors="coerce", dayfirst=False)
df2_clean["AmountUSD"] = pd.to_numeric(df2["Amount"], errors="coerce")
df2_clean["Email"] = df2["Contact"].str.strip().str.lower()

# Concatenate the cleaned dataframes
normalized_df = pd.concat([df1_clean, df2_clean, df3_clean], ignore_index=True)

normalized_df_cleaned = normalized_df.dropna()
print("Info of DataFrame after dropping rows with missing values:")
display(normalized_df_cleaned.info())

print("Missing values per column:")
display(normalized_df.isnull().sum())

print("\nRows with missing 'OrderDate':")
display(normalized_df[normalized_df['OrderDate'].isnull()])

print("\nRows with missing 'AmountUSD':")
display(normalized_df[normalized_df['AmountUSD'].isnull()])

display(normalized_df.describe(include='all'))

display(normalized_df_cleaned.describe(include='all'))

print("Unique Order Dates in normalized_df_cleaned:")
display(normalized_df_cleaned['OrderDate'].unique())

print("\nNumber of orders per OrderDate:")
display(normalized_df_cleaned['OrderDate'].value_counts().sort_index())

import re

# Row Count
row_count = len(normalized_df)

# Null Counts per Column
null_counts = normalized_df.isnull().sum()

# Invalid Emails (basic check for format)
# This regex checks for a basic email pattern: characters@characters.domain
email_regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
invalid_emails = normalized_df[~normalized_df['Email'].str.match(email_regex, na=False)]
invalid_email_count = len(invalid_emails)

# Failed Numeric Conversions (check for non-numeric in original columns that were converted)
# Need to access the original dataframes before conversion to check for failed numeric conversions
# Based on the cleaning steps, AmountUSD was converted from 'AmountUSD' (df1), 'Amount' (df2), and 'ValueUSD' (df3)
# For Source A (df1), 'AmountUSD' was directly converted. We already checked for NaNs after conversion.
# For Source B (df2), 'Amount' was directly converted. We already checked for NaNs after conversion.
# For Source C (df3), 'ValueUSD' was converted after removing commas. We need to check the original 'ValueUSD' for non-numeric after comma removal.

# Check for non-numeric in df3['ValueUSD'] after comma removal but before to_numeric
# This is a simplification; a more robust check would involve examining the original raw data before any cleaning
# However, based on the cleaning code, the primary issue was the "Seven Hundred" entry in df3
failed_amount_conversions_count = normalized_df['AmountUSD'].isnull().sum() - normalized_df['AmountUSD'].dropna().size


# Generate Markdown Table
qc_report = f"""
## Quality Control Report for normalized_df

| Metric | Value |
|---|---|
| Total Rows | {row_count} |
| **Null Counts per Column** | |
| OrderID | {null_counts['OrderID']} |
| CustomerName | {null_counts['CustomerName']} |
| OrderDate | {null_counts['OrderDate']} |
| AmountUSD | {null_counts['AmountUSD']} |
| Email | {null_counts['Email']} |
| Invalid Email Format | {invalid_email_count} |
| Failed AmountUSD Conversions | {failed_amount_conversions_count} |
"""

display(qc_report)

# Create Data Dictionary
data_dict = {
    "Field Name": normalized_df.columns,
    "Data Type": normalized_df.dtypes.astype(str),
    "Description": [
        "Unique identifier for each order.",
        "Name of the customer who placed the order.",
        "Date the order was placed.",
        "Amount of the order in USD.",
        "Email address of the customer."
    ],
    "Transformation Rules": [
        "Concatenated from OrderID (Source A & B) and TransactionID (Source C).",
        "Concatenated from Cust_Name (Source A), Customer (Source B), and ClientName (Source C). Converted to title case and leading/trailing whitespace removed.",
        "Concatenated from OrderDate (Source A), Date (Source B), and TxnDate (Source C). Converted to datetime objects with errors coerced to NaT.",
        "Concatenated from AmountUSD (Source A), Amount (Source B), and ValueUSD (Source C). Converted to numeric with errors coerced to NaN. Commas removed from Source C's ValueUSD.",
        "Concatenated from Email (Source A), Contact (Source B), and EmailAddress (Source C). Converted to lowercase and leading/trailing whitespace removed."
    ]
}

data_dict_df = pd.DataFrame(data_dict)

# Generate Markdown table
markdown_table = data_dict_df.to_markdown(index=False)

# Add a title to the markdown table
markdown_output = "## Data Dictionary for normalized_df\n\n" + markdown_table

display(markdown_output)

import json
import datetime
import hashlib

# Define file name
file_name = "normalized_orders.csv" # Assuming you would save the normalized data to a CSV

# Record Count
record_count = len(normalized_df)

# Schema Version
schema_version = "1.0" # Define a schema version

# Created At Timestamp
created_at = datetime.datetime.now().isoformat()

# Checksum Hash (MD5 of the DataFrame as a string)
# Convert DataFrame to a string for hashing. This might not be the most robust checksum method
# for large datasets or complex data types, but it serves as an example.
df_string = normalized_df.to_string()
checksum_hash = hashlib.md5(df_string.encode('utf-8')).hexdigest()

# Create the manifest dictionary
manifest = {
    "file_name": file_name,
    "record_count": record_count,
    "schema_version": schema_version,
    "created_at": created_at,
    "checksum_hash": checksum_hash
}

# Define the output file path
manifest_file_path = "/content/normalized_orders_manifest.json"

# Save the manifest to a JSON file
with open(manifest_file_path, 'w') as f:
    json.dump(manifest, f, indent=4)

print(f"Manifest file created at: {manifest_file_path}")
display(manifest)

"""## Data Delivery Note

Dear Client,

Please find the normalized order dataset attached/available.

**Transformations Applied:**
Data from Sources A, B, and C has been combined. Key fields (OrderID, CustomerName, OrderDate, AmountUSD, Email) were standardized: names title-cased, emails lowercased, whitespace trimmed, dates converted to datetime, and amounts to numeric, handling conversion errors.

**Key Quality Control Findings:**
Initial data showed missing Order Dates and AmountUSD values, along with some inconsistent email formats. Rows with missing values were removed to ensure data integrity in the delivered dataset. A detailed QC report and data dictionary are provided for your reference.

**Client Readiness:**
The `normalized_df_cleaned` dataset has undergone essential cleaning and validation steps. It is now client-ready for your analysis.

Please let us know if you have any questions.

Sincerely,

Isfaque Ansari
"""

import pandas as pd

# Simulate a billable hours log
data = {
    'Date': pd.to_datetime(['2023-10-02', '2023-10-02', '2023-10-03', '2023-10-03', '2023-10-03', '2023-10-04', '2023-10-04', '2023-10-04', '2023-10-04']),
    'Task': [
        'Profiling Source A',
        'Profiling Source B & C',
        'Normalize Source A',
        'Normalize Source B',
        'Normalize Source C',
        'Concatenate & Handle Missing Values',
        'Generate QC Report',
        'Create Data Dictionary',
        'Generate Data Manifest & Delivery Note'
    ],
    'Hours': [1.0, 1.5, 2.0, 2.0, 2.5, 1.5, 1.0, 1.0, 1.5],
    'Notes': [
        'Initial data exploration and schema identification.',
        'Initial data exploration and schema identification.',
        'Apply cleaning rules for Source A.',
        'Apply cleaning rules for Source B.',
        'Apply cleaning rules for Source C, including handling "Seven Hundred" amount.',
        'Combine dataframes and drop rows with NaNs.',
        'Analyze data quality metrics.',
        'Document schema and transformations.',
        'Prepare summary files for client delivery.'
    ]
}

billable_hours_df = pd.DataFrame(data)

# Format Date column
billable_hours_df['Date'] = billable_hours_df['Date'].dt.strftime('%Y-%m-%d')

# Generate Markdown table
markdown_table = billable_hours_df.to_markdown(index=False)

# Add a title to the markdown table
markdown_output = "## Billable Hours Log\n\n" + markdown_table

display(markdown_output)

